{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing of essential modules/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflowjs as tfjs\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Creating a library, which allows to extract features from files in directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataGenerator:\n",
    "    \n",
    "    \"\"\"Function just for testing. Modify it for your own needs(mainly just path)\"\"\"\n",
    "    def count_files(self):\n",
    "        counter = 0\n",
    "        for directory in os.listdir(\".\\\\dataset3\\\\audio\"):\n",
    "            for file in os.listdir(\".\\\\dataset3\\\\audio\\\\\" + directory):\n",
    "                counter = counter + 1\n",
    "        return counter\n",
    "    \n",
    "    def flow_from_directory(self,dir_path):\n",
    "        file_number = self.count_files()\n",
    "        print(\"Found {} files\".format(file_number))\n",
    "        \n",
    "        \"\"\"All parameters needed for the future\"\"\"\n",
    "        \n",
    "        labels = []\n",
    "        data = []\n",
    "        counter = 0\n",
    "        cat_counter = 0\n",
    "        timeseries_length = 16\n",
    "        hop_length = 512\n",
    "        percentage_counter = 1\n",
    "        step_counter = 1\n",
    "        data = np.zeros((file_number, timeseries_length, 33), dtype=np.float64)\n",
    "          \n",
    "     \n",
    "        \n",
    "        for directory in os.listdir(dir_path):\n",
    "            category = directory\n",
    "            cat_counter = cat_counter + 1\n",
    "            for file in os.listdir(dir_path +\"\\\\\"+category):\n",
    "                y, sr = librosa.load(os.getcwd() + \"\\\\\" + dir_path + \"\\\\\" + category +\"\\\\\" + file)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length= hop_length, n_mfcc= 13)\n",
    "                spectral_center = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length= hop_length)\n",
    "                chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "                spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length= hop_length)\n",
    "                \n",
    "                data[counter, :, 0:13] = mfcc.T[0:timeseries_length, :]\n",
    "                data[counter, :, 13:14] = spectral_center.T[0:timeseries_length, :]\n",
    "                data[counter, :, 14:26] = chroma.T[0:timeseries_length, :]\n",
    "                data[counter, :, 26:33] = spectral_contrast.T[0:timeseries_length, :]\n",
    "            \n",
    "                counter = counter + 1 #represents the number of files that have been extracted\n",
    "                \n",
    "                labels.append(category)\n",
    "                percentage = counter / file_number\n",
    "                \n",
    "                current_step = file_number * 0.05 * step_counter\n",
    "                \n",
    "        \n",
    "                if counter >= current_step:\n",
    "                    clear_output()\n",
    "                    #print(\"{} - {}\".format(counter, current_step)) #Helping line while coding\n",
    "                    self.animate_extraction(percentage)\n",
    "                    step_counter = step_counter + 1\n",
    "        \n",
    "        \n",
    "        print(\"Extracted {} files from {} categories\".format(counter, cat_counter ))\n",
    "        return data, np.expand_dims(np.asarray(labels), axis=1)\n",
    "       \n",
    "    \"\"\"Extracting features of a single file. To test&check how a current model is working\"\"\"    \n",
    "    def test_sample(self, filename):\n",
    "                timeseries_length = 16\n",
    "                hop_length = 512\n",
    "                data = np.zeros((1, timeseries_length, 33), dtype=np.float64)\n",
    "            \n",
    "                y, sr = librosa.load(os.getcwd() + \"\\\\dataset3\\\\test\\\\\"+ filename)\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length= 512, n_mfcc= 13)\n",
    "                spectral_center = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length= hop_length)\n",
    "                chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "                spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length)\n",
    "                \n",
    "                data[0, :, 0:13] = mfcc.T[0:timeseries_length, :]\n",
    "                data[0, :, 13:14] = spectral_center.T[0:timeseries_length, :]\n",
    "                data[0, :, 14:26] = chroma.T[0:timeseries_length, :]\n",
    "                data[0, :, 26:33] = spectral_contrast.T[0:timeseries_length, :]\n",
    "                \n",
    "                \n",
    "                return data\n",
    "            \n",
    "    \"\"\"A simple method to visualise the rate of extracting features.\n",
    "        Not directly related to the meaning of the rest of the code.\"\"\"\n",
    "    \n",
    "    def animate_extraction(self, percentage):\n",
    "        sys.stdout.write(\"[\")\n",
    "        total_num_signs = 20 #Max number of hashes is 20. We refresh the bar every 5%\n",
    "        hash_num = math.floor(percentage*total_num_signs) #Current number of hashes.\n",
    "        \n",
    "        for i in range(0, hash_num):\n",
    "            sys.stdout.write(\"#\")\n",
    "        for i in range(0, total_num_signs - hash_num):\n",
    "            sys.stdout.write(\"-\")\n",
    "\n",
    "        sys.stdout.write(\"] \")\n",
    "        sys.stdout.write(\"{} %\".format(percentage * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Preprocessing data & feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Using function to extract numpy arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################] 100.0 %Extracted 2080 files from 2 categories\n"
     ]
    }
   ],
   "source": [
    "generator = AudioDataGenerator()\n",
    "#generator.count_files() #info about the number of all files(except directories) in all directiories in our path\n",
    "X_train, y_train = generator.flow_from_directory(\"dataset3\\\\audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "lbl_enc = LabelEncoder()\n",
    "\n",
    "y_train_enc = lbl_enc.fit_transform(y_train)\n",
    "y_valid_enc = lbl_enc.transform(y_valid)\n",
    "\n",
    "y_train_enc = to_categorical(y_train_enc, 2)\n",
    "y_valid_enc = to_categorical(y_valid_enc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=128, dropout=0.01, recurrent_dropout=0.1, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(units=64, dropout=0.01, recurrent_dropout=0.1, return_sequences=False))\n",
    "model.add(Dense(units= 128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units= 128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units= 2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1664 samples, validate on 416 samples\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 89s 860ms/step - loss: 0.4874 - acc: 0.7281 - val_loss: 0.5253 - val_acc: 0.7452\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 86s 831ms/step - loss: 0.3448 - acc: 0.8146 - val_loss: 0.6605 - val_acc: 0.6875\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 82s 791ms/step - loss: 0.2875 - acc: 0.8471 - val_loss: 0.7148 - val_acc: 0.7260\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 79s 762ms/step - loss: 0.2486 - acc: 0.8720 - val_loss: 0.7456 - val_acc: 0.6995\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 79s 764ms/step - loss: 0.2139 - acc: 0.8914 - val_loss: 0.8698 - val_acc: 0.7452\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 83s 802ms/step - loss: 0.1919 - acc: 0.9052 - val_loss: 0.8735 - val_acc: 0.7500\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 78s 750ms/step - loss: 0.1709 - acc: 0.9177 - val_loss: 0.8799 - val_acc: 0.7428\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 82s 791ms/step - loss: 0.1503 - acc: 0.9289 - val_loss: 1.1133 - val_acc: 0.7428\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 84s 803ms/step - loss: 0.1362 - acc: 0.9371 - val_loss: 1.0566 - val_acc: 0.7236\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 82s 789ms/step - loss: 0.1241 - acc: 0.9442 - val_loss: 0.9793 - val_acc: 0.7332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6fa15db048>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        X_train,\n",
    "        y_train_enc,\n",
    "        epochs = 10,\n",
    "        steps_per_epoch = 1664// 16,\n",
    "        validation_data = (X_valid, y_valid_enc),\n",
    "        validation_steps = 416 // 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model/yes_or_no.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./model/yes_or_no.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = AudioDataGenerator()\n",
    "test_sample = generator.test_sample(\"myno.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.8801452e-01, 3.5792016e-07]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
