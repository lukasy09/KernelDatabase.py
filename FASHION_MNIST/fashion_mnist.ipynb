{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is coming again. This time, the data contains information what `fashion` looks like. We are going to classify things like: T-Shirts, boots etc (The full list is below and inside the code...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 What does that data looks for humans..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul style = \"list-style-type: none;\">\n",
       "    <li style = \"float:left;\"><img src =\"./assets/1.png\" width = 220px /></li>\n",
       "    <li style = \"float:left\"><img src =\"./assets/2.png\" width = 220px /></li>\n",
       "    <li style = \"float:left\"><img src =\"./assets/3.png\" width = 220px /></li>\n",
       "    <li style = \"float:left\"><img src =\"./assets/4.png\" width = 220px /></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<ul style = \"list-style-type: none;\">\n",
    "    <li style = \"float:left;\"><img src =\"./assets/1.png\" width = 220px /></li>\n",
    "    <li style = \"float:left\"><img src =\"./assets/2.png\" width = 220px /></li>\n",
    "    <li style = \"float:left\"><img src =\"./assets/3.png\" width = 220px /></li>\n",
    "    <li style = \"float:left\"><img src =\"./assets/4.png\" width = 220px /></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Possible object labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 T-shirt/top\n",
    "1 Trouser\n",
    "2 Pullover\n",
    "3 Dress\n",
    "4 Coat\n",
    "5 Sandal\n",
    "6 Shirt\n",
    "7 Sneaker\n",
    "8 Bag\n",
    "9 Ankle boot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Well-known library for uploading data & preprocessing\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"Visualising the data\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\"ML backend\"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"Keras dependencies for building model\"\"\"\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.utils.np_utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"./data/fashion-mnist_train.csv\") # 28x28 images, 784 pixels\n",
    "dataframe_test = pd.read_csv(\"./data/fashion-mnist_test.csv\")  # Same like above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe.iloc[:, 1:].values # Data to train & validate during training\n",
    "Y = dataframe['label'].values #Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling data/ Normalisation [0 - 1] range & Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255\n",
    "X = X.reshape(-1,28,28,1)\n",
    "Y = to_categorical(Y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=10,\n",
    "    zoom_range = 0.1,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip= False,\n",
    "    vertical_flip = False\n",
    ")\n",
    "\n",
    "data_generator.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Layers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Conv2D(32, (5,5),activation = \"relu\",padding = 'Same',input_shape = (28,28,1)),\n",
    "            Conv2D(32, (5,5),activation = \"relu\",padding = 'Same'),\n",
    "            MaxPool2D(pool_size = (2, 2)),\n",
    "            Dropout(0.25),\n",
    "            Conv2D(64, (3,3), activation = \"relu\", padding ='Same'),\n",
    "            Conv2D(64, (3,3),activation = \"relu\", padding = 'Same'),\n",
    "            MaxPool2D(pool_size = (2, 2), strides = (2,2)),\n",
    "            Dropout(0.25),\n",
    "            Flatten(),\n",
    "            Dense(units = 256, activation = \"relu\"),\n",
    "            Dense(units = 10, activation = \"softmax\") ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compilation, choosing loss function, optimizers and other parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "421/421 [==============================] - 382s 907ms/step - loss: 0.8230 - acc: 0.6914 - val_loss: 0.4968 - val_acc: 0.8045\n",
      "Epoch 2/30\n",
      "421/421 [==============================] - 392s 930ms/step - loss: 0.5706 - acc: 0.7841 - val_loss: 0.4339 - val_acc: 0.8312\n",
      "Epoch 3/30\n",
      "421/421 [==============================] - 386s 918ms/step - loss: 0.4931 - acc: 0.8139 - val_loss: 0.3667 - val_acc: 0.8618\n",
      "Epoch 4/30\n",
      "421/421 [==============================] - 396s 940ms/step - loss: 0.4420 - acc: 0.8345 - val_loss: 0.3196 - val_acc: 0.8805\n",
      "Epoch 5/30\n",
      "421/421 [==============================] - 394s 935ms/step - loss: 0.4063 - acc: 0.8471 - val_loss: 0.3153 - val_acc: 0.8820\n",
      "Epoch 6/30\n",
      "421/421 [==============================] - 395s 939ms/step - loss: 0.3851 - acc: 0.8550 - val_loss: 0.2879 - val_acc: 0.8913\n",
      "Epoch 7/30\n",
      "421/421 [==============================] - 394s 935ms/step - loss: 0.3748 - acc: 0.8599 - val_loss: 0.2980 - val_acc: 0.8885\n",
      "Epoch 8/30\n",
      "421/421 [==============================] - 395s 938ms/step - loss: 0.3595 - acc: 0.8641 - val_loss: 0.2714 - val_acc: 0.8985\n",
      "Epoch 9/30\n",
      "421/421 [==============================] - 396s 940ms/step - loss: 0.3472 - acc: 0.8690 - val_loss: 0.2772 - val_acc: 0.8960\n",
      "Epoch 10/30\n",
      "421/421 [==============================] - 397s 944ms/step - loss: 0.3387 - acc: 0.8751 - val_loss: 0.2821 - val_acc: 0.8958\n",
      "Epoch 11/30\n",
      "421/421 [==============================] - 1012s 2s/step - loss: 0.3309 - acc: 0.8753 - val_loss: 0.2708 - val_acc: 0.9035\n",
      "Epoch 12/30\n",
      "421/421 [==============================] - 472s 1s/step - loss: 0.3269 - acc: 0.8777 - val_loss: 0.2557 - val_acc: 0.9095\n",
      "Epoch 13/30\n",
      "421/421 [==============================] - 393s 934ms/step - loss: 0.3213 - acc: 0.8806 - val_loss: 0.2642 - val_acc: 0.9090\n",
      "Epoch 14/30\n",
      "421/421 [==============================] - 388s 921ms/step - loss: 0.3157 - acc: 0.8825 - val_loss: 0.2496 - val_acc: 0.9085\n",
      "Epoch 15/30\n",
      "421/421 [==============================] - 409s 972ms/step - loss: 0.3106 - acc: 0.8819 - val_loss: 0.2478 - val_acc: 0.9082\n",
      "Epoch 16/30\n",
      "421/421 [==============================] - 421s 999ms/step - loss: 0.3043 - acc: 0.8861 - val_loss: 0.2389 - val_acc: 0.9103\n",
      "Epoch 17/30\n",
      "421/421 [==============================] - 402s 955ms/step - loss: 0.2994 - acc: 0.8876 - val_loss: 0.2434 - val_acc: 0.9097\n",
      "Epoch 18/30\n",
      "421/421 [==============================] - 391s 930ms/step - loss: 0.2975 - acc: 0.8881 - val_loss: 0.2340 - val_acc: 0.9148\n",
      "Epoch 19/30\n",
      "421/421 [==============================] - 396s 941ms/step - loss: 0.2953 - acc: 0.8898 - val_loss: 0.2403 - val_acc: 0.9138\n",
      "Epoch 20/30\n",
      "421/421 [==============================] - 394s 937ms/step - loss: 0.2920 - acc: 0.8910 - val_loss: 0.2280 - val_acc: 0.9152\n",
      "Epoch 21/30\n",
      "421/421 [==============================] - 397s 942ms/step - loss: 0.2854 - acc: 0.8937 - val_loss: 0.2201 - val_acc: 0.9205\n",
      "Epoch 22/30\n",
      "421/421 [==============================] - 400s 951ms/step - loss: 0.2874 - acc: 0.8921 - val_loss: 0.2352 - val_acc: 0.9162\n",
      "Epoch 23/30\n",
      "421/421 [==============================] - 410s 973ms/step - loss: 0.2818 - acc: 0.8942 - val_loss: 0.2333 - val_acc: 0.9122\n",
      "Epoch 24/30\n",
      "421/421 [==============================] - 409s 971ms/step - loss: 0.2842 - acc: 0.8934 - val_loss: 0.2268 - val_acc: 0.9188\n",
      "Epoch 25/30\n",
      "421/421 [==============================] - 395s 939ms/step - loss: 0.2775 - acc: 0.8959 - val_loss: 0.2207 - val_acc: 0.9202\n",
      "Epoch 26/30\n",
      "421/421 [==============================] - 403s 957ms/step - loss: 0.2756 - acc: 0.8976 - val_loss: 0.2401 - val_acc: 0.9158\n",
      "Epoch 27/30\n",
      "421/421 [==============================] - 397s 942ms/step - loss: 0.2736 - acc: 0.8987 - val_loss: 0.2408 - val_acc: 0.9115\n",
      "Epoch 28/30\n",
      "421/421 [==============================] - 403s 958ms/step - loss: 0.2753 - acc: 0.8966 - val_loss: 0.2384 - val_acc: 0.9105\n",
      "Epoch 29/30\n",
      "421/421 [==============================] - 392s 932ms/step - loss: 0.2708 - acc: 0.9001 - val_loss: 0.2356 - val_acc: 0.9167\n",
      "Epoch 30/30\n",
      "421/421 [==============================] - 409s 972ms/step - loss: 0.2701 - acc: 0.8986 - val_loss: 0.2178 - val_acc: 0.9198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xcbeab29630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    data_generator.flow(X_train, Y_train, batch_size = 128),\n",
    "    epochs = 30,\n",
    "    steps_per_epoch = X_train.shape[0] // 128,\n",
    "    validation_data = (X_valid, Y_valid)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Saving to an external file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 .h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjs.converters.save_keras_model(model, './model_json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
